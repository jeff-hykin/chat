[config]
model = "onnx-community/Llama-3.2-1B-Instruct" # Model to use
system = [
  "You are an expert in computer science and programming.",
  "Provide detailed explanations and code examples when necessary."
] # System prompts
max_new_tokens = 128 # Maximum number of tokens to generate
max_length = 20 # Maximum length of the response
temperature = 1.0 # Temperature for sampling
top_p = 1.0 # Top-p for sampling
repetition_penalty = 1.2 # Repetition penalty